{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Example 4 — Earnings Call Transcripts\n",
    "\n",
    "**Goal:** Download structured earnings call transcripts from Motley Fool by ticker symbol, extract speakers, prepared remarks, and Q&A sections, and save to Parquet.\n",
    "\n",
    "The `transcripts` subcommand is different from `search` and `crawl`:\n",
    "\n",
    "| | Search | Crawl | Transcripts |\n",
    "|---|---|---|---|\n",
    "| Input | Keyword queries | Seed URLs | Ticker symbols |\n",
    "| Discovery | DuckDuckGo results | BFS link-following | Motley Fool sitemaps |\n",
    "| Content | General articles | General articles + PDFs | Earnings call transcripts |\n",
    "| Output | Parquet | Parquet | Parquet (same schema) |\n",
    "\n",
    "Use `transcripts` when you need:\n",
    "- Full earnings call transcripts for specific companies\n",
    "- Structured data: speakers, prepared remarks, Q&A sections\n",
    "- Historical transcripts by fiscal year and quarter\n",
    "\n",
    "> **No extra dependencies required.** The transcripts module uses `requests` and `beautifulsoup4` (both already installed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Make sure the package is installed:\n",
    "\n",
    "```bash\n",
    "cd financial_scraper\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from financial_scraper.transcripts import TranscriptConfig, TranscriptPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Configure and Run\n",
    "\n",
    "Key settings for transcript downloading:\n",
    "\n",
    "| Setting | Value | Why |\n",
    "|---------|-------|-----|\n",
    "| `tickers` | `(\"AAPL\", \"MSFT\")` | Companies to download transcripts for |\n",
    "| `year` | `2025` | Fiscal year (default: current year) |\n",
    "| `quarters` | `(\"Q1\",)` | Filter to Q1 only (empty = all quarters) |\n",
    "\n",
    "### How discovery works\n",
    "\n",
    "1. Scans Motley Fool monthly XML sitemaps (`fool.com/sitemap/YYYY/MM`)\n",
    "2. Filters URLs matching the ticker in the URL slug (e.g. `-aapl-q1-2025-earnings`)\n",
    "3. Scans target year + following year (Q4 transcripts often published in Jan/Feb)\n",
    "4. Fetches each transcript page with polite 1.5s delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./runs_transcripts_example\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_parquet = output_dir / \"transcripts.parquet\"\n",
    "output_jsonl = output_dir / \"transcripts.jsonl\"\n",
    "\n",
    "config = TranscriptConfig(\n",
    "    tickers=(\"AAPL\", \"MSFT\"),\n",
    "    year=2025,\n",
    "    quarters=(\"Q1\",),          # Empty tuple = all quarters\n",
    "    output_dir=output_dir,\n",
    "    output_path=output_parquet,\n",
    "    jsonl_path=output_jsonl,\n",
    "    checkpoint_file=output_dir / \".checkpoint.json\",\n",
    ")\n",
    "\n",
    "print(\"Config ready:\")\n",
    "print(f\"  tickers   = {config.tickers}\")\n",
    "print(f\"  year      = {config.year}\")\n",
    "print(f\"  quarters  = {config.quarters}\")\n",
    "print(f\"  output    = {config.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TranscriptPipeline(config)\n",
    "pipeline.run()  # Synchronous — no asyncio.run() needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_parquet.exists():\n",
    "    df = pd.read_parquet(output_parquet)\n",
    "    print(f\"Total transcripts: {len(df)}\")\n",
    "    print(f\"Unique tickers:    {df['company'].nunique()}\")\n",
    "    print(f\"Total words:       {df['full_text'].str.split().str.len().sum():,}\")\n",
    "    print(f\"Avg words/doc:     {df['full_text'].str.split().str.len().mean():.0f}\")\n",
    "    print()\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "else:\n",
    "    print(\"No output file — check logs above for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview all rows\n",
    "if output_parquet.exists():\n",
    "    df[[\"company\", \"title\", \"source\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count per transcript\n",
    "if output_parquet.exists():\n",
    "    print(\"Words per transcript:\")\n",
    "    for _, row in df.iterrows():\n",
    "        words = len(row[\"full_text\"].split())\n",
    "        print(f\"  {row['company']} {row['title'].split('Transcript')[0].strip()}: {words:,} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first transcript\n",
    "if output_parquet.exists() and len(df) > 0:\n",
    "    row = df.iloc[0]\n",
    "    print(f\"Ticker: {row['company']}\")\n",
    "    print(f\"Title:  {row['title']}\")\n",
    "    print(f\"Date:   {row['date']}\")\n",
    "    print(f\"Source: {row['source']}\")\n",
    "    print(f\"Words:  {len(row['full_text'].split())}\")\n",
    "    print(f\"\\n--- First 1000 chars ---\\n\")\n",
    "    print(row[\"full_text\"][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Extract Structured Content\n",
    "\n",
    "The transcript extractor can also return structured data (speakers, prepared remarks, Q&A). Let's use the extraction module directly on the raw HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from financial_scraper.transcripts.extract import extract_transcript\n",
    "\n",
    "# Fetch a transcript page directly\n",
    "if output_parquet.exists() and len(df) > 0:\n",
    "    url = df.iloc[0][\"link\"]\n",
    "    print(f\"Fetching: {url}\\n\")\n",
    "    resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=30)\n",
    "    result = extract_transcript(resp.text)\n",
    "\n",
    "    if result:\n",
    "        print(f\"Company:  {result.company}\")\n",
    "        print(f\"Ticker:   {result.ticker}\")\n",
    "        print(f\"Quarter:  {result.quarter}\")\n",
    "        print(f\"Year:     {result.year}\")\n",
    "        print(f\"Date:     {result.date}\")\n",
    "        print(f\"\\nParticipants ({len(result.participants)}):\")\n",
    "        for p in result.participants[:10]:\n",
    "            print(f\"  - {p}\")\n",
    "        print(f\"\\nSpeakers ({len(result.speakers)}):\")\n",
    "        for s in result.speakers:\n",
    "            print(f\"  - {s}\")\n",
    "        print(f\"\\nPrepared remarks: {len(result.prepared_remarks.split())} words\")\n",
    "        print(f\"Q&A section:      {len(result.qa_section.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Using a Ticker File\n",
    "\n",
    "For many tickers, use a file (one ticker per line, `#` comments allowed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_path = Path(\"tickers_example.txt\")\n",
    "tickers_path.write_text(\n",
    "    \"\"\"# Big Tech\n",
    "AAPL\n",
    "MSFT\n",
    "GOOG\n",
    "AMZN\n",
    "META\n",
    "NVDA\n",
    "\"\"\"\n",
    ")\n",
    "print(f\"Wrote {tickers_path} with 6 tickers\")\n",
    "print()\n",
    "print(\"To run from CLI:\")\n",
    "print(\"  financial-scraper transcripts --tickers-file tickers_example.txt --year 2025 --output-dir ./runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. CLI Equivalent\n",
    "\n",
    "The same run from the command line:\n",
    "\n",
    "```bash\n",
    "financial-scraper transcripts \\\n",
    "    --tickers AAPL MSFT \\\n",
    "    --year 2025 \\\n",
    "    --quarters Q1 \\\n",
    "    --output-dir ./runs_transcripts_example \\\n",
    "    --jsonl\n",
    "```\n",
    "\n",
    "All quarters for a single ticker:\n",
    "\n",
    "```bash\n",
    "financial-scraper transcripts \\\n",
    "    --tickers NVDA \\\n",
    "    --year 2025 \\\n",
    "    --output-dir ./runs\n",
    "```\n",
    "\n",
    "Resume an interrupted download:\n",
    "\n",
    "```bash\n",
    "financial-scraper transcripts \\\n",
    "    --tickers AAPL MSFT GOOG AMZN META NVDA \\\n",
    "    --year 2025 \\\n",
    "    --resume \\\n",
    "    --output-dir ./runs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete temporary files\n",
    "# tickers_path.unlink(missing_ok=True)\n",
    "# import shutil; shutil.rmtree(output_dir, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
