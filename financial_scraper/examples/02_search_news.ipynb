{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 — News Search Mode\n",
    "\n",
    "**Goal:** Search DuckDuckGo in **news** mode for recent financial articles, extract clean text with deduplication (exact + fuzzy), and analyze the results.\n",
    "\n",
    "News mode returns recent articles from news outlets. It is best suited for:\n",
    "- Earnings announcements and market reactions\n",
    "- Commodity price movements and supply updates\n",
    "- Central bank decisions and economic data releases\n",
    "- Breaking financial events\n",
    "\n",
    "> **Tip:** News mode is **less rate-limited** by DuckDuckGo than text mode. It is the recommended mode for financial content scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "import pandas as pd\n",
    "from financial_scraper import ScraperConfig, ScraperPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a Query File\n",
    "\n",
    "News queries work best with 4-8 words focusing on a specific event or topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_path = Path(\"queries_news_example.txt\")\n",
    "queries_path.write_text(\n",
    "    \"\"\"# News search — recent financial articles\n",
    "crude oil futures price outlook\n",
    "gold price safe haven demand\n",
    "Federal Reserve interest rate decision\n",
    "nvidia earnings revenue AI demand\n",
    "wheat commodity market supply\n",
    "natural gas storage winter forecast\n",
    "copper demand electric vehicle battery\n",
    "\"\"\"\n",
    ")\n",
    "print(f\"Wrote {queries_path} with 7 queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure and Run\n",
    "\n",
    "Key settings for news search:\n",
    "\n",
    "| Setting | Value | Why |\n",
    "|---------|-------|-----|\n",
    "| `search_type` | `\"news\"` | Recent articles from news outlets — less rate-limited |\n",
    "| `max_results_per_query` | `15` | Good coverage without being excessive |\n",
    "| `min_word_count` | `80` | Lower threshold — news articles can be shorter |\n",
    "| `date_from` | `\"2025-01-01\"` | Filter out stale articles (optional) |\n",
    "| `resume` | `True` | Safe to re-run if interrupted |\n",
    "\n",
    "### Deduplication layers\n",
    "\n",
    "The pipeline automatically applies two dedup layers:\n",
    "1. **Exact** — SHA256 of first 2000 chars (catches identical republished articles)\n",
    "2. **Fuzzy** — MinHash LSH (catches syndicated rewrites with minor edits, added disclaimers, different headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./runs_news_example\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_parquet = output_dir / \"news_search_results.parquet\"\n",
    "output_jsonl = output_dir / \"news_search_results.jsonl\"\n",
    "\n",
    "config = ScraperConfig(\n",
    "    queries_file=queries_path,\n",
    "    search_type=\"news\",\n",
    "    max_results_per_query=15,\n",
    "    min_word_count=80,\n",
    "    favor_precision=True,\n",
    "    # Optional date filtering — keeps only articles after this date\n",
    "    date_from=\"2025-01-01\",\n",
    "    output_dir=output_dir,\n",
    "    output_path=output_parquet,\n",
    "    jsonl_path=output_jsonl,\n",
    "    # Resume support — safe to re-run\n",
    "    resume=True,\n",
    "    checkpoint_file=output_dir / \".checkpoint.json\",\n",
    "    # Domain exclusion: uses built-in list by default\n",
    "    exclude_file=Path(\"../config/exclude_domains.txt\"),\n",
    ")\n",
    "\n",
    "print(\"Config ready:\")\n",
    "print(f\"  search_type    = {config.search_type}\")\n",
    "print(f\"  max_results    = {config.max_results_per_query}\")\n",
    "print(f\"  min_words      = {config.min_word_count}\")\n",
    "print(f\"  date_from      = {config.date_from}\")\n",
    "print(f\"  resume         = {config.resume}\")\n",
    "print(f\"  output         = {config.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ScraperPipeline(config)\n",
    "await pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_parquet.exists():\n",
    "    df = pd.read_parquet(output_parquet)\n",
    "    print(f\"Total documents: {len(df)}\")\n",
    "    print(f\"Unique sources:  {df['source'].nunique()}\")\n",
    "    print(f\"Unique queries:  {df['company'].nunique()}\")\n",
    "    print(f\"Total words:     {df['full_text'].str.split().str.len().sum():,}\")\n",
    "    print(f\"Avg words/doc:   {df['full_text'].str.split().str.len().mean():.0f}\")\n",
    "else:\n",
    "    print(\"No output file — check logs above for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first rows\n",
    "if output_parquet.exists():\n",
    "    df[[\"company\", \"title\", \"source\", \"date\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top domains by article count\n",
    "if output_parquet.exists():\n",
    "    print(\"Top sources:\")\n",
    "    print(df[\"source\"].value_counts().head(15).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results per query\n",
    "if output_parquet.exists():\n",
    "    print(\"Results per query:\")\n",
    "    for query, group in df.groupby(\"company\"):\n",
    "        avg_words = group[\"full_text\"].str.split().str.len().mean()\n",
    "        print(f\"  {query}: {len(group)} articles, {avg_words:.0f} avg words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date distribution\n",
    "if output_parquet.exists():\n",
    "    dated = df.dropna(subset=[\"date\"])\n",
    "    print(f\"Articles with dates: {len(dated)}/{len(df)}\")\n",
    "    if len(dated) > 0:\n",
    "        print(f\"Date range: {dated['date'].min()} to {dated['date'].max()}\")\n",
    "        print(\"\\nArticles per month:\")\n",
    "        print(dated[\"date\"].dt.to_period(\"M\").value_counts().sort_index().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample article\n",
    "if output_parquet.exists() and len(df) > 0:\n",
    "    row = df.iloc[0]\n",
    "    print(f\"Title:  {row['title']}\")\n",
    "    print(f\"Source: {row['source']}\")\n",
    "    print(f\"Query:  {row['company']}\")\n",
    "    print(f\"Date:   {row['date']}\")\n",
    "    print(f\"Words:  {len(row['full_text'].split())}\")\n",
    "    print(f\"\\n--- First 800 chars ---\\n\")\n",
    "    print(row[\"full_text\"][:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CLI Equivalent\n",
    "\n",
    "The same run from the command line:\n",
    "\n",
    "```bash\n",
    "financial-scraper search \\\n",
    "    --queries-file queries_news_example.txt \\\n",
    "    --search-type news \\\n",
    "    --max-results 15 \\\n",
    "    --min-words 80 \\\n",
    "    --date-from 2025-01-01 \\\n",
    "    --resume \\\n",
    "    --output-dir ./runs_news_example \\\n",
    "    --jsonl\n",
    "```\n",
    "\n",
    "For a larger commodity run (50 queries) with stealth:\n",
    "\n",
    "```bash\n",
    "financial-scraper search \\\n",
    "    --queries-file config/commodities_50.txt \\\n",
    "    --search-type news \\\n",
    "    --max-results 20 \\\n",
    "    --stealth --resume \\\n",
    "    --output-dir ./runs \\\n",
    "    --exclude-file config/exclude_domains.txt \\\n",
    "    --jsonl\n",
    "```\n",
    "\n",
    "For 300+ queries, add Tor:\n",
    "\n",
    "```bash\n",
    "financial-scraper search \\\n",
    "    --queries-file config/commodities_300.txt \\\n",
    "    --search-type news \\\n",
    "    --stealth --use-tor --resume \\\n",
    "    --output-dir ./runs \\\n",
    "    --jsonl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete temporary files\n",
    "# queries_path.unlink(missing_ok=True)\n",
    "# import shutil; shutil.rmtree(output_dir, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
