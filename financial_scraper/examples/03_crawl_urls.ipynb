{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3 — Deep-Crawl Specific URLs\n",
    "\n",
    "**Goal:** Deep-crawl a list of seed URLs using a headless browser (crawl4ai), discover internal links via BFS financial-keyword scoring, extract content from HTML and PDFs, and save to Parquet.\n",
    "\n",
    "The `crawl` subcommand is fundamentally different from `search`:\n",
    "\n",
    "| | Search | Crawl |\n",
    "|---|---|---|\n",
    "| Input | Keyword queries | Seed URLs |\n",
    "| Discovery | DuckDuckGo results | BFS link-following on the seed domain |\n",
    "| Renderer | Simple HTTP (aiohttp) | Headless browser (crawl4ai) |\n",
    "| JS support | No | Yes |\n",
    "| PDF handling | pdfplumber | pdfplumber or Docling |\n",
    "\n",
    "Use `crawl` when you:\n",
    "- Know the exact URLs or domains you want\n",
    "- Need JavaScript rendering (SPAs, dynamic dashboards)\n",
    "- Want to discover internal links on a corporate/investor site\n",
    "- Need to extract PDFs (annual reports, SEC filings)\n",
    "\n",
    "> **Prerequisite:** Install the crawl extra: `pip install -e \".[crawl]\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Note: do NOT set WindowsSelectorEventLoopPolicy for crawl —\n",
    "# crawl4ai uses Playwright which needs the default event loop.\n",
    "\n",
    "import pandas as pd\n",
    "from financial_scraper.crawl.config import CrawlConfig\n",
    "from financial_scraper.crawl.pipeline import CrawlPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a Seed URL File\n",
    "\n",
    "One URL per line. These can be:\n",
    "- HTML pages (the crawler will render them and follow links)\n",
    "- Direct PDF URLs (downloaded and extracted automatically)\n",
    "- Investor relations landing pages (the BFS scorer prioritizes financial links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_path = Path(\"seed_urls_example.txt\")\n",
    "urls_path.write_text(\n",
    "    \"\"\"# Seed URLs for deep crawl\n",
    "# Corporate investor pages — the crawler will follow internal links\n",
    "https://investor.apple.com/sec-filings/default.aspx\n",
    "https://ir.tesla.com\n",
    "\n",
    "# Direct PDF — downloaded and extracted automatically\n",
    "https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/aapl-20230930.htm\n",
    "\"\"\"\n",
    ")\n",
    "print(f\"Wrote {urls_path} with 3 seed URLs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure and Run\n",
    "\n",
    "Key settings for URL crawling:\n",
    "\n",
    "| Setting | Value | Why |\n",
    "|---------|-------|-----|\n",
    "| `max_depth` | `2` | Follow links 2 levels deep from each seed |\n",
    "| `max_pages` | `20` | Cap pages per seed URL (prevents runaway crawls) |\n",
    "| `semaphore_count` | `2` | Max concurrent browser tabs |\n",
    "| `pdf_extractor` | `\"auto\"` | Uses Docling if installed, falls back to pdfplumber |\n",
    "| `min_word_count` | `100` | Filter out navigation/stub pages |\n",
    "\n",
    "### How BFS scoring works\n",
    "\n",
    "Discovered links are scored before the crawler decides which to visit:\n",
    "- **Financial keyword relevance** (weight 0.8) — URLs containing `earnings`, `quarterly`, `report`, `sec`, `filings` score higher\n",
    "- **Path depth** (weight 0.3) — shorter paths preferred\n",
    "- **Freshness** (weight 0.15) — URLs containing the current year score higher\n",
    "\n",
    "Non-content URLs (login, contact, career, legal) are filtered out before scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"./runs_crawl_example\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_parquet = output_dir / \"crawl_results.parquet\"\n",
    "output_jsonl = output_dir / \"crawl_results.jsonl\"\n",
    "\n",
    "config = CrawlConfig(\n",
    "    urls_file=urls_path,\n",
    "    max_depth=2,\n",
    "    max_pages=20,\n",
    "    semaphore_count=2,\n",
    "    pdf_extractor=\"auto\",\n",
    "    min_word_count=100,\n",
    "    output_dir=output_dir,\n",
    "    output_path=output_parquet,\n",
    "    jsonl_path=output_jsonl,\n",
    "    # Resume support — safe to re-run\n",
    "    resume=True,\n",
    "    checkpoint_file=output_dir / \".crawl_checkpoint.json\",\n",
    "    # Domain exclusion: uses built-in list by default (set exclude_file=None to disable)\n",
    "    exclude_file=Path(\"../config/exclude_domains.txt\"),\n",
    ")\n",
    "\n",
    "print(\"Config ready:\")\n",
    "print(f\"  urls_file      = {config.urls_file}\")\n",
    "print(f\"  max_depth      = {config.max_depth}\")\n",
    "print(f\"  max_pages      = {config.max_pages}\")\n",
    "print(f\"  pdf_extractor  = {config.pdf_extractor}\")\n",
    "print(f\"  output         = {config.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = CrawlPipeline(config)\n",
    "await pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_parquet.exists():\n",
    "    df = pd.read_parquet(output_parquet)\n",
    "    print(f\"Total documents: {len(df)}\")\n",
    "    print(f\"Unique sources:  {df['source'].nunique()}\")\n",
    "    print(f\"Total words:     {df['full_text'].str.split().str.len().sum():,}\")\n",
    "    print(f\"Avg words/doc:   {df['full_text'].str.split().str.len().mean():.0f}\")\n",
    "else:\n",
    "    print(\"No output file — check logs above for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview extracted pages\n",
    "if output_parquet.exists():\n",
    "    df[[\"company\", \"title\", \"source\", \"date\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pages per seed domain\n",
    "if output_parquet.exists():\n",
    "    print(\"Pages per seed domain:\")\n",
    "    print(df[\"company\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content type breakdown (HTML vs PDF, based on link extension)\n",
    "if output_parquet.exists():\n",
    "    df[\"is_pdf\"] = df[\"link\"].str.lower().str.endswith(\".pdf\")\n",
    "    pdf_count = df[\"is_pdf\"].sum()\n",
    "    html_count = len(df) - pdf_count\n",
    "    print(f\"HTML pages: {html_count}\")\n",
    "    print(f\"PDF files:  {pdf_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count distribution\n",
    "if output_parquet.exists():\n",
    "    word_counts = df[\"full_text\"].str.split().str.len()\n",
    "    print(f\"Word count stats:\")\n",
    "    print(f\"  Min:    {word_counts.min()}\")\n",
    "    print(f\"  Median: {word_counts.median():.0f}\")\n",
    "    print(f\"  Mean:   {word_counts.mean():.0f}\")\n",
    "    print(f\"  Max:    {word_counts.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document\n",
    "if output_parquet.exists() and len(df) > 0:\n",
    "    row = df.iloc[0]\n",
    "    print(f\"Title:  {row['title']}\")\n",
    "    print(f\"Source: {row['source']}\")\n",
    "    print(f\"Link:   {row['link']}\")\n",
    "    print(f\"Date:   {row['date']}\")\n",
    "    print(f\"Words:  {len(row['full_text'].split())}\")\n",
    "    print(f\"\\n--- First 800 chars ---\\n\")\n",
    "    print(row[\"full_text\"][:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CLI Equivalent\n",
    "\n",
    "The same run from the command line:\n",
    "\n",
    "```bash\n",
    "financial-scraper crawl \\\n",
    "    --urls-file seed_urls_example.txt \\\n",
    "    --max-depth 2 \\\n",
    "    --max-pages 20 \\\n",
    "    --resume \\\n",
    "    --output-dir ./runs_crawl_example \\\n",
    "    --jsonl\n",
    "```\n",
    "\n",
    "Use Docling for better PDF extraction (tables, layout):\n",
    "\n",
    "```bash\n",
    "pip install -e \".[docling]\"\n",
    "\n",
    "financial-scraper crawl \\\n",
    "    --urls-file seed_urls.txt \\\n",
    "    --max-depth 2 \\\n",
    "    --max-pages 50 \\\n",
    "    --pdf-extractor docling \\\n",
    "    --resume \\\n",
    "    --output-dir ./runs \\\n",
    "    --jsonl\n",
    "```\n",
    "\n",
    "Stealth mode (lower concurrency):\n",
    "\n",
    "```bash\n",
    "financial-scraper crawl \\\n",
    "    --urls-file seed_urls.txt \\\n",
    "    --max-depth 3 \\\n",
    "    --max-pages 100 \\\n",
    "    --stealth --resume \\\n",
    "    --output-dir ./runs \\\n",
    "    --jsonl\n",
    "```\n",
    "\n",
    "To disable domain exclusions:\n",
    "\n",
    "```bash\n",
    "financial-scraper crawl --urls-file seed_urls.txt --no-exclude\n",
    "```\n",
    "\n",
    "> **Windows note:** If you see Unicode encoding errors from crawl4ai's logger, set `PYTHONUTF8=1` before running:\n",
    "> ```bash\n",
    "> set PYTHONUTF8=1\n",
    "> python -m financial_scraper crawl --urls-file seed_urls.txt --output-dir ./runs\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete temporary files\n",
    "# urls_path.unlink(missing_ok=True)\n",
    "# import shutil; shutil.rmtree(output_dir, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
